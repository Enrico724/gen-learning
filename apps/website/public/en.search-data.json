{"/docs/":{"data":{"":"Benvenuto nella sezione di documentazione! In questa pagina esploreremo l’infrastruttura che supporta i nostri sistemi, le tecnologie utilizzate e il flusso di dati che collega i vari componenti del nostro ecosistema. Il diagramma seguente fornisce una panoramica visiva dell’architettura.","flusso-di-dati#Flusso di Dati":"Il flusso di dati è orchestrato attraverso Kafka, che funge da hub centrale. I microservizi consumano e producono messaggi, garantendo un’elaborazione modulare e asincrona. I dati vengono inoltre inviati allo stack ELK per il monitoraggio.\nQuesta documentazione ti guiderà attraverso ogni componente, fornendo dettagli tecnici per comprendere e utilizzare al meglio il sistema.","infrastruttura#Infrastruttura":"flowchart LR; %% === Componenti del Flusso === api_gateway[API Gateway] kafka[Kafka] subgraph \"Microservizi\" prompt_enricher[\"prompt-enricher\"] instructional_designer[\"instructional-designer\"] paragraph_enricher[\"paragraph-enricher\"] book_aggregator[\"book-aggregator\"] book_producer[\"book-producer\"] end %% Nuovi componenti a destra subgraph \"Processing \u0026 AI\" spark[Spark] gemini[Gemini API] end %% Stack di Logging e Monitoring subgraph \"Monitoring \u0026 Logging\" logstash[Logstash] elastic[Elasticsearch] kibana[Kibana] end %% === Connessioni del Flusso Principale === %% API Gateway produce in Kafka api_gateway --\u003e kafka %% Microservizi che consumano da Kafka e producono verso Spark kafka --\u003e prompt_enricher prompt_enricher --\u003e spark kafka --\u003e instructional_designer instructional_designer --\u003e spark kafka --\u003e paragraph_enricher paragraph_enricher --\u003e spark %% Spark chiama Gemini API spark --\u003e gemini %% Altri microservizi che interagiscono con Kafka kafka \u003c--\u003e book_aggregator kafka --\u003e book_producer %% === Connessioni verso lo Stack di Logging === kafka --\u003e logstash %% === Connessioni interne dello Stack ELK === logstash --\u003e elastic elastic --\u003e kibana L’infrastruttura è stata progettata per garantire scalabilità, affidabilità e sicurezza. I principali componenti includono:\nAPI Gateway: punto di ingresso per le richieste, che vengono instradate verso i microservizi. Kafka: sistema di messaggistica centrale per la comunicazione asincrona tra i componenti. Microservizi: ciascuno con responsabilità specifiche, come l’arricchimento dei dati e l’aggregazione dei contenuti. Stack di Logging e Monitoring (ELK): per il monitoraggio e l’analisi dei log.","tecnologie-utilizzate#Tecnologie Utilizzate":"Per costruire e gestire questa infrastruttura, utilizziamo:\nDocker e Kubernetes: per il deployment e la gestione dei container. Linguaggi di programmazione: Python, Typescript, Javascript e Java per il backend, Flutter ed HUGO per il frontend. Sistemi di logging: Logstash, Elasticsearch e Kibana per il monitoraggio."},"title":"Docs"},"/docs/apps/":{"data":{"":"","panoramica#Panoramica":"La sezione Apps offre una panoramica dei vari componenti che costituiscono il sistema:\nAPI Gateway: Funziona come hub centrale per la gestione e l’instradamento delle richieste API. App Flutter: Un’applicazione mobile sviluppata con Flutter per garantire compatibilità multipiattaforma. CLI: Uno strumento a riga di comando per sviluppatori e utenti avanzati. Sito Web: La piattaforma che stai attualmente navigando, che offre documentazione e risorse. Ogni componente svolge un ruolo cruciale per garantire un’esperienza utente senza interruzioni."},"title":"_index"},"/docs/apps/api-gateway/":{"data":{"1-routing-delle-richieste#1. \u003cstrong\u003eRouting delle richieste\u003c/strong\u003e":"Instradamento delle richieste verso i servizi backend appropriati. Supporto per regole di routing basate su URL, intestazioni o parametri.","2-trasformazione-delle-richieste-e-risposte#2. \u003cstrong\u003eTrasformazione delle Richieste e Risposte\u003c/strong\u003e":"Supporto esclusivo per il formato JSON. Validazione automatica dei dati in ingresso.","api-gateway#API Gateway":"API Gateway graph LR; %% Inizio del flusso request[Richiesta Utente] --\u003e api_gateway[API Gateway]; %% L'API Gateway invia i dati a due destinazioni api_gateway --\u003e kafka[Kafka]; api_gateway --\u003e mongodb[MongoDB];","descrizione#Descrizione":"L’API Gateway, sviluppato con il framework NestJS, funge da punto di ingresso centrale per tutte le richieste verso i servizi backend. Consente di gestire, monitorare e ottimizzare il traffico API, migliorando la sicurezza e le prestazioni.","funzionalità-principali#Funzionalità principali":"","installazione#Installazione":"Questo progetto si trova all’interno della monorepo nella directory apps/api-gateway.\nPer installare le dipendenze, eseguire: npm install Per avviare l’applicazione, eseguire: npm run start:dev"},"title":"api-gateway"},"/docs/apps/app/":{"data":{"descrizione-dellapp#Descrizione dell\u0026rsquo;App":"Descrizione dell’AppQuesta applicazione è progettata per semplificare la gestione delle attività quotidiane. Offre agli utenti la possibilità di creare, organizzare e monitorare le proprie attività, rendendo più semplice e intuitivo il raggiungimento degli obiettivi personali.\nNota: Questa applicazione è in fase di sviluppo utilizzando Flutter e fa parte di un progetto di tesi.","funzionalità-principali#Funzionalità principali":"Creazione di attività: Permette di aggiungere nuove attività con dettagli personalizzati. Organizzazione per categorie: Consente di suddividere le attività in categorie per una gestione più efficiente. Monitoraggio dei progressi: Fornisce statistiche e aggiornamenti sul completamento delle attività.","obiettivo#Obiettivo":"L’obiettivo principale dell’app è aiutare gli utenti a migliorare la produttività e a gestire meglio il proprio tempo.\nPer ulteriori dettagli, consulta la documentazione completa."},"title":"app"},"/docs/apps/cli/":{"data":{"documentazione-dellapp-cli#Documentazione dell\u0026rsquo;App CLI":"Documentazione dell’App CLI","panoramica#Panoramica":"L’app CLI si trova in apps/cli all’interno di questa monorepo. Avviala con:\npython apps/cli Consulta la figura per un esempio."},"title":"cli"},"/docs/dataflow/":{"data":{"":"","panoramica-dei-dataflow#Panoramica dei Dataflow":"I dataflow rappresentano i processi automatizzati per la gestione e l’elaborazione dei dati. Attualmente, è disponibile il dataflow Genbbok, che si occupa di:\nRaccolta dei dati da fonti specifiche. Trasformazione e normalizzazione dei dati. Caricamento dei dati nei sistemi di destinazione. In futuro, verranno aggiunti ulteriori dataflow per coprire altre esigenze di elaborazione e integrazione dei dati."},"title":"_index"},"/docs/dataflow/genbook/":{"data":{"documentazione-del-flusso-di-dati-di-genbook#Documentazione del Flusso di Dati di GenBook":"flowchart TD subgraph Interazione con l'Utente A[App / CLI] --\u003e|Crea Richiesta GenBook| B[Gateway API] end subgraph Nucleo del Sistema B --\u003e|Genera ID Lavoro, Crea Stato GenBook| C[MongoDB] B --\u003e|Pubblica Lavoro su Kafka| D[Topic Kafka: book-queue] end subgraph Pipeline di Elaborazione Kafka D --\u003e|Consuma Lavoro| E[prompt-enricher] E --\u003e|Espande il prompt con link| E E --\u003e|Pubblica prompt arricchito| F[Topic Kafka: enriched_prompts] F --\u003e|Consuma prompt arricchito| G[instructional-designer] G --\u003e|Genera struttura libro e info arricchimento paragrafi| G G --\u003e|Pubblica struttura libro| H[Topic Kafka: book-structure] G --\u003e|Pubblica info arricchimento paragrafi| I[Topic Kafka: paragraph-enrichment] I --\u003e|Consuma info arricchimento paragrafi| J[paragraph-enricher] J --\u003e|Genera testo Markdown dalle info| J J --\u003e|Pubblica paragrafo arricchito| K[Topic Kafka: enriched-paragraphs] end subgraph Aggregazione Finale H --\u003e|Consuma struttura libro| L[book-aggregator] K --\u003e|Consuma paragrafi arricchiti| L L --\u003e|Aggrega e costruisce il libro finale| L L --\u003e|Pubblica libro finale| M[Topic Kafka: generated-books] M --\u003e|Aggiorna Stato GenBook in MongoDB| C end subgraph Archiviazione dei Dati e Logging C -- Stato GenBook --\u003e X{Dati} subgraph Stack ELK E --\u003e Z[Logstash] G --\u003e Z J --\u003e Z L --\u003e Z Z --\u003e Y[Elasticsearch] end end Documentazione del Flusso di Dati di GenBook","panoramica#Panoramica":"Questo documento descrive il flusso di dati del sistema GenBook, basandosi sul diagramma del flusso di dati. Il sistema è suddiviso in quattro sezioni principali:\nInterazione con l’Utente: L’utente interagisce con il sistema tramite un’applicazione o CLI, avviando la creazione di una richiesta GenBook che viene gestita dal Gateway API. Nucleo del Sistema: Il Gateway API genera un ID lavoro unico, aggiorna lo stato in MongoDB e pubblica il lavoro su un topic Kafka per l’elaborazione. Pipeline di Elaborazione Kafka: Una serie di microservizi elabora i dati, arricchendo i prompt, generando la struttura del libro e i paragrafi arricchiti. Aggregazione Finale e Archiviazione: I dati vengono aggregati per costruire il libro finale, pubblicati su un topic Kafka e archiviati in MongoDB. I log vengono raccolti dallo stack ELK per il monitoraggio. Ogni componente e processo è rappresentato nel diagramma sopra."},"title":"genbook"},"/docs/infrastruttura/":{"data":{"":"","architettura-modulare#Architettura Modulare":"L’architettura modulare consente di:\nScalare i singoli componenti in base alle necessità. Semplificare la manutenzione e l’integrazione di nuove funzionalità. Garantire un’elaborazione efficiente e affidabile. Questa infrastruttura è progettata per supportare un flusso di lavoro robusto e scalabile, con un’attenzione particolare al monitoraggio e alla tracciabilità dei dati.","descrizione-dellinfrastruttura#Descrizione dell\u0026rsquo;Infrastruttura":"L’infrastruttura del progetto è composta da tre principali sezioni:\nAPI Gateway e Kafka Topics:\nL’API Gateway funge da punto di ingresso per le richieste, che vengono instradate verso il topic Kafka book-queue. I dati attraversano una serie di topic Kafka (enriched-prompt, book-structures, paragraphs, enriched-paragraphs, generated-books) che fungono da buffer e punti di scambio tra i microservizi. Microservizi:\nI microservizi sono organizzati per gestire specifiche funzionalità: prompt-enricher: Arricchisce i prompt ricevuti. instructional-designer: Suddivide i dati in strutture di libro e paragrafi. paragraph-enricher: Arricchisce i paragrafi generati. book-aggregator: Combina le strutture e i paragrafi arricchiti. book-producer: Produce i libri finali. Monitoring \u0026 Logging:\nLo stack ELK (Logstash, Elasticsearch, Kibana) è utilizzato per il monitoraggio e la registrazione dei log. I dati di log vengono inviati da vari componenti a Logstash, che li inoltra a Elasticsearch per l’archiviazione. Kibana fornisce un’interfaccia per la visualizzazione.","flusso-dei-dati#Flusso dei Dati":"Le richieste iniziano dall’API Gateway e vengono elaborate attraverso una pipeline di microservizi. I dati vengono arricchiti, suddivisi, aggregati e infine trasformati in libri generati. Durante il processo, i log vengono raccolti e analizzati per garantire il monitoraggio continuo.","infrastruttura-del-progetto#Infrastruttura del Progetto":"flowchart LR; %% === Componenti del Flusso === api_gateway[API Gateway] subgraph \"Kafka Topics\" book_queue[\"book-queue\"] enriched_prompt[\"enriched-prompt\"] book_structures[\"book-structures\"] paragraphs[\"paragraphs\"] enriched_paragraphs[\"enriched-paragraphs\"] generated_books[\"generated-books\"] end subgraph \"Microservizi\" prompt_enricher[\"prompt-enricher\"] instructional_designer[\"instructional-designer\"] paragraph_enricher[\"paragraph-enricher\"] book_aggregator[\"book-aggregator\"] book_producer[\"book-producer\"] end %% Stack di Logging e Monitoring subgraph \"Monitoring \u0026 Logging\" logstash[Logstash] elastic[Elasticsearch] kibana[Kibana] end %% === Connessioni del Flusso Principale === %% API Gateway -\u003e prompt-enricher api_gateway --\u003e book_queue book_queue --\u003e prompt_enricher prompt_enricher --\u003e enriched_prompt %% instructional-designer (Split) enriched_prompt --\u003e instructional_designer instructional_designer --\u003e book_structures instructional_designer --\u003e paragraphs %% paragraph-enricher paragraphs --\u003e paragraph_enricher paragraph_enricher --\u003e enriched_paragraphs %% book-aggregator (Join) book_structures --\u003e book_aggregator enriched_paragraphs --\u003e book_aggregator %% book-producer (Fine del flusso) book_aggregator --\u003e generated_books generated_books --\u003e book_producer %% === Connessioni verso lo Stack di Logging === enriched_prompt --\u003e logstash book_structures --\u003e logstash enriched_paragraphs --\u003e logstash %% === Connessioni interne dello Stack ELK === logstash --\u003e elastic elastic --\u003e kibana"},"title":"_index"},"/docs/infrastruttura/elk/":{"data":{"":"","conclusione#Conclusione":"Lo stack ELK è una soluzione completa per la gestione e l’analisi dei dati. Il flusso descritto nel grafico rappresenta un esempio di come i dati possono essere trasformati e analizzati per ottenere informazioni utili.","descrizione-del-flusso#Descrizione del Flusso":"Il grafico sopra rappresenta il flusso dei dati all’interno dello stack ELK (Elasticsearch, Logstash, Kibana). Ecco una descrizione dettagliata di ogni componente e del loro ruolo:\nTopics di Input:\nenriched-prompts: Contiene i dati relativi ai prompt arricchiti. enriched-paragraphs: Contiene i paragrafi arricchiti. book-structures: Contiene le strutture dei libri. Questi dati rappresentano i punti di ingresso nel sistema.\nLogstash:\nLogstash è il servizio responsabile dell’elaborazione e della trasformazione dei dati in ingresso. Riceve i dati dai topics di input e li prepara per l’indicizzazione. Elasticsearch:\nElasticsearch è il motore di ricerca e analisi che memorizza i dati elaborati da Logstash. Consente di eseguire ricerche rapide e analisi sui dati. Kibana:\nKibana è l’interfaccia utente che consente di visualizzare e analizzare i dati memorizzati in Elasticsearch. Fornisce dashboard interattivi e strumenti di visualizzazione.","flusso-dati#Flusso Dati":"graph LR; %% Definiamo i nodi di input (topics) topic1(enriched-prompts); topic2(enriched-paragraphs); topic3(book-structures); %% Definiamo i servizi logstash_service[Logstash]; elasticsearch_service[Elasticsearch]; kibana_service[Kibana]; %% Definiamo il flusso topic1 --\u003e logstash_service; topic2 --\u003e logstash_service; topic3 --\u003e logstash_service; logstash_service --\u003e elasticsearch_service; elasticsearch_service --\u003e kibana_service;","utilizzo#Utilizzo":"Questo flusso è utile per gestire grandi quantità di dati strutturati e non strutturati, consentendo di trasformarli, indicizzarli e analizzarli in modo efficiente. Lo stack ELK è comunemente utilizzato per il monitoraggio, l’analisi dei log e la business intelligence.","vantaggi#Vantaggi":"Scalabilità: Lo stack ELK può gestire grandi volumi di dati. Flessibilità: Supporta diversi tipi di dati e consente personalizzazioni. Visualizzazione Intuitiva: Kibana offre strumenti di visualizzazione potenti e facili da usare."},"title":"ELK Stack"},"/docs/infrastruttura/kafka/":{"data":{"":"","flusso-lavorativo#Flusso lavorativo":"graph LR; %% Start api_gateway[API Gateway]; %% Topics (ovals) t1(book-queue); t2(enriched-prompt); t3(book-structures); t4(paragraphs); t5(enriched-paragraphs); t6(generated-books); %% Microservices (rectangles) ms1[prompt-enricher]; ms2[instructional-designer]; ms3[paragraph-enricher]; ms4[book-aggregator]; ms5[book-producer]; %% Flow api_gateway --\u003e t1; t1 --\u003e ms1; ms1 --\u003e t2; t2 --\u003e ms2; ms2 --\u003e t3; ms2 --\u003e t4; t4 --\u003e ms3; ms3 --\u003e t5; t3 --\u003e ms4; t5 --\u003e ms4; ms4 --\u003e t6; t6 --\u003e ms5; Il flusso lavorativo illustra come i dati si muovono attraverso i vari microservizi e i topic di Kafka. Ecco una descrizione passo-passo del processo:\nAPI Gateway: Riceve le richieste iniziali e le invia al topic book-queue. Prompt Enricher: Consuma i messaggi da book-queue, arricchisce i prompt e li pubblica su enriched-prompt. Instructional Designer: Consuma i prompt arricchiti da enriched-prompt e genera sia le strutture dei libri (book-structures) che i paragrafi (paragraphs). Paragraph Enricher: Consuma i paragrafi da paragraphs, li arricchisce e li pubblica su enriched-paragraphs. Book Aggregator: Combina le strutture dei libri (book-structures) e i paragrafi arricchiti (enriched-paragraphs) per creare libri completi, che vengono pubblicati su generated-books. Book Producer: Consuma i libri completi da generated-books e li rende disponibili per ulteriori utilizzi o distribuzione. Questo flusso garantisce una pipeline modulare e scalabile per la generazione e l’arricchimento dei contenuti. Ogni microservizio è responsabile di un compito specifico, facilitando la manutenzione e l’estensibilità del sistema.","topic-di-kafka#Topic di Kafka":"graph LR; %% Riquadro per raggruppare i topic di Kafka subgraph KAFKA direction TB; %% Lista dei topic (ovals) t1(book-queue); t2(enriched-prompt); t3(book-structures); t4(paragraphs); t5(enriched-paragraphs); t6(generated-books); end I topic di Kafka rappresentano i canali di comunicazione utilizzati per lo scambio di messaggi tra i vari microservizi. Ogni topic è identificato da un nome univoco e funge da buffer per i dati che vengono prodotti e consumati dai servizi. Nel diagramma sopra, i topic sono rappresentati come ovali e sono raggruppati all’interno del riquadro “KAFKA”.\nEcco una breve descrizione dei topic:\nbook-queue: Contiene le richieste iniziali inviate dall’API Gateway. enriched-prompt: Memorizza i prompt arricchiti dal microservizio prompt-enricher. book-structures: Contiene le strutture dei libri generate dal microservizio instructional-designer. paragraphs: Raccoglie i paragrafi generati dal microservizio instructional-designer. enriched-paragraphs: Contiene i paragrafi arricchiti dal microservizio paragraph-enricher. generated-books: Memorizza i libri completi generati dal microservizio book-aggregator."},"title":"kafka"},"/docs/infrastruttura/spark/":{"data":{"":"graph LR; subgraph \"Sorgente Dati\" kafka[Kafka] end subgraph \"Microservizi Driver Spark\" prompt_enricher[\"prompt-enricher\"] instructional_designer[\"instructional-designer\"] paragraph_enricher[\"paragraph-enricher\"] end subgraph \"Processing \u0026 AI\" spark[Spark] gemini[Gemini API] end %% Flusso da Kafka ai microservizi kafka \u003c--\u003e prompt_enricher; kafka \u003c--\u003e instructional_designer; kafka \u003c--\u003e paragraph_enricher; %% Flusso dai microservizi a Spark prompt_enricher --\u003e spark; instructional_designer --\u003e spark; paragraph_enricher --\u003e spark; %% Flusso da Spark a Gemini spark --\u003e gemini;","documentazione-spark#Documentazione Spark":"Questo documento descrive l’architettura e il flusso di dati che coinvolgono Spark all’interno del nostro sistema. La configurazione utilizza Kafka 4.0.0 come sorgente dati e PySpark per l’elaborazione.","flusso-dei-dati#Flusso dei Dati":"Sorgente Dati: I dati vengono acquisiti da Kafka, che funge da sistema di messaggistica distribuito. Microservizi Driver Spark: I dati vengono elaborati da tre microservizi: prompt-enricher: Arricchisce i dati con informazioni aggiuntive. instructional-designer: Progetta istruzioni basate sui dati. paragraph-enricher: Migliora i paragrafi con contenuti generati. Processing \u0026 AI: I dati elaborati dai microservizi vengono inviati a Spark per ulteriori trasformazioni e analisi. Infine, i risultati vengono inviati all’API Gemini per l’integrazione con i sistemi di intelligenza artificiale.","note#Note":"Assicurarsi che i microservizi siano configurati per leggere e scrivere correttamente da e verso Kafka. Verificare che Spark sia configurato per ricevere input dai microservizi e inviare output all’API Gemini. Questa architettura garantisce un’elaborazione scalabile e modulare dei dati, sfruttando le potenzialità di Kafka e Spark.","tecnologie-utilizzate#Tecnologie Utilizzate":"Kafka 4.0.0: Utilizzato per la gestione dei flussi di dati in tempo reale. PySpark: Libreria Python per l’elaborazione distribuita dei dati con Apache Spark."},"title":"spark"},"/docs/microservizi/":{"data":{"architettura-dei-microservizi#Architettura dei Microservizi":"Di seguito è riportato un diagramma che illustra i microservizi e le loro interconnessioni:\ngraph TD; %% Definiamo i nodi dei microservizi prompt_enricher[\"prompt-enricher\"]; instructional_designer[\"instructional-designer\"]; paragraph_enricher[\"paragraph-enricher\"]; book_aggregator[\"book-aggregator\"]; book_producer[\"book-producer\"]; %% Definiamo le connessioni tra i microservizi %% Flusso iniziale prompt_enricher --\u003e instructional_designer; %% Split del flusso instructional_designer --\u003e paragraph_enricher; instructional_designer --\u003e book_aggregator; %% Join del flusso paragraph_enricher --\u003e book_aggregator; %% Flusso finale book_aggregator --\u003e book_producer;","book-aggregator#Book Aggregator":"Il Book Aggregator raccoglie i contenuti generati dai microservizi precedenti e li aggrega in un formato coerente.","book-producer#Book Producer":"Il microservizio finale, book-producer, si occupa di produrre il risultato finale, come un libro o un documento completo.","descrizione-dei-microservizi#Descrizione dei Microservizi":"","flusso-dei-dati#Flusso dei Dati":"Il flusso dei dati inizia con il prompt-enricher, passa attraverso i vari microservizi e termina con il book-producer. Questo approccio modulare garantisce flessibilità e scalabilità, consentendo di aggiornare o sostituire singoli microservizi senza influire sull’intero sistema.","instructional-designer#Instructional Designer":"Il microservizio Instructional Designer elabora i prompt arricchiti e genera istruzioni dettagliate per i passaggi successivi.","overview-dei-microservizi#Overview dei Microservizi":"Overview dei MicroserviziQuesta pagina fornisce una panoramica dei microservizi che compongono il sistema. Ogni microservizio è progettato per svolgere un compito specifico, contribuendo al flusso complessivo del sistema.","paragraph-enricher#Paragraph Enricher":"Questo microservizio arricchisce i paragrafi generati, migliorandone la qualità e la coerenza.","prompt-enricher#Prompt Enricher":"Questo microservizio si occupa di arricchire i prompt iniziali, preparando i dati per il flusso successivo."},"title":"_index"},"/docs/microservizi/book-aggregator/":{"data":{"":"","descrizione-del-flusso#Descrizione del Flusso":"Il diagramma sopra rappresenta il flusso dati del microservizio book-aggregator. Questo microservizio si occupa di aggregare e trasformare i dati provenienti da due topic di input per generare un output consolidato.\nInput:\nbook-structures: Contiene le strutture base dei libri. enriched-paragraphs: Contiene paragrafi arricchiti con metadati o contenuti aggiuntivi. Elaborazione:\nIl microservizio book-aggregator riceve i dati dai due topic di input e li combina per creare una rappresentazione completa e coerente del libro. Output:\ngenerated-books: Il risultato finale è un topic che contiene i libri generati pronti per essere utilizzati da altri sistemi o microservizi.","dettagli-implementativi#Dettagli Implementativi":"Tecnologie Utilizzate: Il microservizio è implementato utilizzando Java e si basa su Spark Kafka Streaming.","flusso-dati#Flusso Dati":"graph LR; %% Definiamo i nodi %% La sintassi (testo) crea un nodo con angoli arrotondati (ovale) topic_in1(book-structures); topic_in2(enriched-paragraphs); microservice[book-aggregator]; topic_out(generated-books); %% Definiamo il flusso topic_in1 --\u003e microservice; topic_in2 --\u003e microservice; microservice --\u003e topic_out;","scalabilità-e-monitoraggio#Scalabilità e Monitoraggio":"Scalabilità: Il sistema sfrutta Apache Spark per l’elaborazione distribuita e Apache Kafka per la gestione dei topic, garantendo la capacità di gestire grandi volumi di dati attraverso meccanismi di partizionamento e bilanciamento del carico. Monitoraggio: Per garantire il corretto funzionamento del flusso e identificare eventuali problemi, è stato implementato un sistema di monitoraggio basato su ELK (Elasticsearch, Logstash, e Kibana)."},"title":"book-aggregator"},"/docs/microservizi/book-producer/":{"data":{"":"","dettagli-del-flusso#Dettagli del Flusso":"Input: Il microservizio riceve i dati dal topic generated-books. Elaborazione: I dati vengono processati dal microservizio book-producer. Output: Il risultato dell’elaborazione è un file PDF generato. Questo microservizio è un componente chiave per la produzione di documenti PDF automatizzati.","flusso-dati#Flusso Dati":"Il microservizio book-producer è responsabile della generazione di file PDF a partire dai dati ricevuti. Il flusso dei dati è rappresentato nel diagramma seguente:\ngraph LR; %% Definiamo i nodi %% La sintassi (testo) crea un nodo con angoli arrotondati (ovale) topic_in(generated-books); microservice[book-producer]; %% Definiamo il flusso topic_in --\u003e microservice;"},"title":"Book Producer"},"/docs/microservizi/instructional-designer/":{"data":{"":"","flusso-dati#Flusso Dati":"graph LR; %% Definiamo i nodi topic_in[\"enriched-prompt\"]; microservice[\"instructional-designer\"]; topic_out1[\"book-structures\"]; topic_out2[\"paragraphs\"]; %% Definiamo il flusso topic_in --\u003e microservice; microservice --\u003e topic_out1; microservice --\u003e topic_out2; Da Kafka riceviamo\n{ \"job_id\": \"String\", \"enriched_prompt\": \"String\", // Altri campi non rilevanti per il microservizio vengono ignorati } Per poi ottenere dopo un’elaborazione un dato che inseriremo nel topic di output di Kafka per le strutture del libro.\n{ \"titolo_libro\": \"String\", \"capitoli\": [ { \"titolo_capitolo\": \"String\", \"sottocapitoli\": [ \"UUID\", \"UUID\", \"UUID\", \"...\" ] } ], \"metrics\": { \"chapter_qty\": \"Integer\", \"subchapter_qty\": \"Integer\", \"paragraphs\": \"Integer\", \"duration_ms\": \"Integer\" } } Mentre sul topic dei paragrafi da generare\n{ \"paragraph_id\": \"String\", \"titolo_paragrafo\": \"String\", \"testo_segnaposto\": \"String\", \"enrichment_info\": { \"focus\": \"String\", \"tipo_arricchimento\": [\"String\"], \"parole_chiave\": [\"String\"], \"domande_guida\": [\"String\"], \"livello_dettaglio\": \"String\" } }"},"title":"instructional-designer"},"/docs/microservizi/paragraph-enricher/":{"data":{"flusso-dati#Flusso Dati":"Flusso Dati graph LR; %% Definiamo i nodi %% La sintassi (testo) crea un nodo con angoli arrotondati (ovale) topic_in(paragraphs); microservice[paragraph-enricher]; topic_out(enriched-paragraphs); %% Definiamo il flusso topic_in --\u003e microservice; microservice --\u003e topic_out; Da Kafka riceviamo\n{ \"paragraph_id\": \"String\", \"titolo_paragrafo\": \"String\", \"testo_segnaposto\": \"String\", \"enrichment_info\": { \"focus\": \"String\", \"tipo_arricchimento\": [\"String\"], \"parole_chiave\": [\"String\"], \"domande_guida\": [\"String\"], \"livello_dettaglio\": \"String\" } } Dopo l’elaborazione, otteniamo un dato che viene inserito direttamente nel topic di output di Kafka e che include anche le metriche relative al paragrafo generato.\n{ \"paragraph_id\": \"String\", \"enriched_paragraph\": \"String\", \"metrics\": { \"duration_ms\": \"Integer\", \"paragraph_length\": \"Integer\" } }"},"title":"paragraph-enricher"},"/docs/microservizi/prompt-enricher/":{"data":{"":"","flusso-dati#Flusso Dati":"graph LR; %% Definiamo i nodi: due topic e un microservizio topic_in[\"topic: book-queue\"]; microservice[\"microservizio: prompt-enricher\"]; topic_out[\"topic: enriched-prompts\"]; %% Definiamo il flusso topic_in --\u003e microservice; microservice --\u003e topic_out; Da Kafka riceviamo\n{ \"job_id\": \"String\", \"prompt\": \"String\", \"timestamp\": \"String\" } Per poi ottenere dopo un’elaborazione un dato che inseriremo nel topic di output di Kafka.\n{ \"job_id\": \"String\", \"enriched_prompt\": \"String\", \"duration_ms\": \"Integer\", \"metrics\": { \"original_prompt_length\": \"Integer\", \"enriched_prompt_length\": \"Integer\", \"links_processed\": \"Integer\", \"links_failed\": \"Integer\" }, \"error\": \"String\" } Il topic enriched-prompts conterrà quindi tutti i dati elaborati dal microservizio."},"title":"prompt-enricher"}}